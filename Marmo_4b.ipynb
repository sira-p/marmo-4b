{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sira-p/marmo-4b/blob/main/Marmo_4b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part One: Transfer Learning from Pre-trained Pose Detection Model"
      ],
      "metadata": {
        "id": "HAylxwWD8a8j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQPMyPl3gtlf"
      },
      "outputs": [],
      "source": [
        "#download training dataset\n",
        "!rm -rf sample_data\n",
        "!test -d train || (curl -o train.zip https://zenodo.org/records/5849371/files/marmoset-dlc-2021-05-07.zip?download=1 && unzip -q train.zip -d ./train && rm -f train.zip)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#install ultralytics and deeplabcut2yolo packages via pip\n",
        "!pip install ultralytics deeplabcut2yolo"
      ],
      "metadata": {
        "id": "A1gT6k6ih637"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#obsolete\n",
        "def mount_drive():\n",
        "  import locale\n",
        "  def getpreferredencoding(do_setlocale = True):\n",
        "      return \"UTF-8\"\n",
        "  locale.getpreferredencoding = getpreferredencoding\n",
        "\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "mount_drive()"
      ],
      "metadata": {
        "id": "MtFmh5NhX0e3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load deeplabcut packages and specify input-output path\n",
        "import deeplabcut2yolo as d2y\n",
        "\n",
        "json_path = \"./train/marmoset-dlc-2021-05-07/training-datasets/iteration-0/UnaugmentedDataSet_marmosetMay7/dlc_shuffle1_train.json\"\n",
        "csv_path = \"./train/marmoset-dlc-2021-05-07/training-datasets/iteration-0/UnaugmentedDataSet_marmosetMay7/CollectedData_dlc.csv\"\n",
        "root_dir = \"./train/marmoset-dlc-2021-05-07/labeled-data/\"\n",
        "\n",
        "#initiate conversion from deeplabcut(coco-like structure to yolov8 format)\n",
        "d2y.convert(json_path, csv_path, root_dir, datapoint_classes=[0, 1], n_keypoint_per_datapoint=30, precision=6)"
      ],
      "metadata": {
        "id": "wojuMs2_h-H6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define function for create configuration file (.yaml)\n",
        "def create_data_yml(train_path, val_path, kpt_shape, flip_idx, nc, names, output_path):\n",
        "  data = {\n",
        "    \"train\": train_path,\n",
        "    \"val\": val_path,\n",
        "    \"kpt_shape\": kpt_shape,\n",
        "    \"flip_idx\": flip_idx,\n",
        "    \"nc\": nc,\n",
        "    \"names\": names\n",
        "  }\n",
        "\n",
        "  with open(output_path, \"w\") as f:\n",
        "    import yaml\n",
        "    yaml.dump(data, f, default_flow_style=None, sort_keys=False)"
      ],
      "metadata": {
        "id": "BgCTgTkMiAEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining variables for file path and configuration for configuration file (.yaml)\n",
        "train_path = \"/content/train/marmoset-dlc-2021-05-07/labeled-data/reachingvideo1\"\n",
        "val_path = \"/content/train/marmoset-dlc-2021-05-07/labeled-data/refinement1\"\n",
        "data_path = \"/content/data.yml\"\n",
        "shape = [15, 3]\n",
        "flip_index = [0, 3, 2, 1, 6, 7, 4, 5, 9, 8, 11, 10, 12, 13, 14]\n",
        "nc = 2\n",
        "names = [\"B\", \"W\"]\n",
        "\n",
        "#loading function create_data_yml to create configuration file (.yaml)\n",
        "create_data_yml(train_path, val_path, shape, flip_index, nc, names, data_path)"
      ],
      "metadata": {
        "id": "l2dnIeZ6iCQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading YOLOv8 packages\n",
        "from ultralytics import YOLO\n",
        "\n",
        "#load YOLOv8 pose estimation pretrained model\n",
        "model = YOLO(\"yolov8n-pose.pt\").load('yolov8n-pose.pt')\n",
        "#fine tune the pretrained model with the marmoset dataset from deeplabcut\n",
        "model.train(data=\"data.yml\", epochs=50, imgsz=640)"
      ],
      "metadata": {
        "id": "TUDfesiniE4h",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#move files to a new directory\n",
        "!mv /content/runs/pose/train /content/drive/MyDrive/marmo-4b/"
      ],
      "metadata": {
        "id": "-CLuql7I76wl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optionally, train a one-class pose detection model."
      ],
      "metadata": {
        "id": "V2sJhhpWZDvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load deeplabcut packages and specify input-output path\n",
        "import deeplabcut2yolo as d2y\n",
        "\n",
        "json_path = \"./train/marmoset-dlc-2021-05-07/training-datasets/iteration-0/UnaugmentedDataSet_marmosetMay7/dlc_shuffle1_train.json\"\n",
        "csv_path = \"./train/marmoset-dlc-2021-05-07/training-datasets/iteration-0/UnaugmentedDataSet_marmosetMay7/CollectedData_dlc.csv\"\n",
        "root_dir = \"./train/marmoset-dlc-2021-05-07/labeled-data/\"\n",
        "\n",
        "#initiate conversion from deeplabcut(coco-like structure to yolov8 format)\n",
        "d2y.convert(json_path, csv_path, root_dir, datapoint_classes=[0, 0], n_keypoint_per_datapoint=30, precision=6)"
      ],
      "metadata": {
        "id": "qeAq4sViZ-1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining variables for file path and configuration for configuration file (.yaml)\n",
        "train_path = \"/content/train/marmoset-dlc-2021-05-07/labeled-data/reachingvideo1\"\n",
        "val_path = \"/content/train/marmoset-dlc-2021-05-07/labeled-data/refinement1\"\n",
        "data_path = \"/content/data.yml\"\n",
        "shape = [15, 3]\n",
        "flip_index = [0, 3, 2, 1, 6, 7, 4, 5, 9, 8, 11, 10, 12, 13, 14]\n",
        "\n",
        "#initiate conversion from deeplabcut(coco-like structure to yolov8 format) by specifying class as 1, the model is trained only with one marmoset\n",
        "create_data_yml(train_path, val_path, shape, flip_index, 1, [\"marmoset\"], data_path)"
      ],
      "metadata": {
        "id": "0Fucfa_zaR7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading YOLOv8 packages\n",
        "from ultralytics import YOLO\n",
        "\n",
        "#load YOLOv8 pose estimation pretrained model\n",
        "model_one_class = YOLO(\"yolov8n-pose.pt\").load('yolov8n-pose.pt')\n",
        "#fine tune the pretrained model with the marmoset dataset from deeplabcut\n",
        "model_one_class.train(data=\"data.yml\", epochs=50, imgsz=640)"
      ],
      "metadata": {
        "id": "zbv7hHAGZM2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#move trained files to a new directory\n",
        "!mv /content/runs/pose/train /content/drive/MyDrive/marmo-4b-one/"
      ],
      "metadata": {
        "id": "1bm9Ky0tajrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part Two: SAM Mask Generation Using Points from Pose Detection"
      ],
      "metadata": {
        "id": "4oOArwGY8l8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#workaround for notebook git clone bug\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "#download pretrained pose detection model (from part 1) from github\n",
        "!git clone https://github.com/sira-p/marmo-4b.git\n",
        "!rm -rf sample_data\n",
        "#download segmentanything from github\n",
        "!find sam_vit_h.pth || curl -o sam_vit_h.pth https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
        "#download the test dataset\n",
        "!test -d test || (curl -o test.zip https://zenodo.org/records/8437121/files/marmoset-dlc-2021-05-07.zip?download=1 && unzip -q test.zip -d ./test && rm -f test.zip)\n",
        "#install segmentanything and ultralytics from pip\n",
        "!pip install git+https://github.com/facebookresearch/segment-anything.git\n",
        "!pip install ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etYF8ekQ8poQ",
        "outputId": "a7807a2a-bafe-4c00-fa9a-27d6db4fcfec"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'marmo-4b'...\n",
            "remote: Enumerating objects: 38, done.\u001b[K\n",
            "remote: Counting objects: 100% (38/38), done.\u001b[K\n",
            "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
            "remote: Total 38 (delta 12), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (38/38), 17.59 MiB | 15.55 MiB/s, done.\n",
            "Resolving deltas: 100% (12/12), done.\n",
            "sam_vit_h.pth\n",
            "Collecting git+https://github.com/facebookresearch/segment-anything.git\n",
            "  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-ltor72w1\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-ltor72w1\n",
            "  Resolved https://github.com/facebookresearch/segment-anything.git to commit 526fd066dea338ba2ca08886853bd37ffd6a8aec\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.10/dist-packages (8.2.90)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.19.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.1.4)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.6)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loading dependecies\n",
        "from ultralytics import YOLO\n",
        "import glob\n",
        "from segment_anything import SamPredictor, sam_model_registry\n",
        "import numpy as np\n",
        "import torch\n",
        "import cv2\n",
        "import os\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "nHkd4lv5fTaA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define the Marmo-4b model class\n",
        "class Marmo4b:\n",
        "  #defining the color for the masks and skeletons connections\n",
        "  COLORS = [\n",
        "      (255, 0, 0),\n",
        "      (0, 255, 0),\n",
        "      (0, 0, 255),\n",
        "      (255, 255, 0),\n",
        "      (255, 0, 255),\n",
        "      (0, 255, 255),\n",
        "    ]\n",
        "  SKELETON = [\n",
        "    (0, 1),\n",
        "    (0, 2),\n",
        "    (0, 3),\n",
        "    (3, 7),\n",
        "    (7, 8),\n",
        "    (5, 12),\n",
        "    (13, 14),\n",
        "    (11, 14),\n",
        "    (3, 9),\n",
        "    (9, 10),\n",
        "    (3, 4),\n",
        "    (4, 5),\n",
        "    (6, 13),\n",
        "    (6, 11),\n",
        "  ]\n",
        "  CONFIDENCE_THRESHOLD = 0.5\n",
        "\n",
        "#defining initiation function for SAM\n",
        "  def __init__(self, pose_model_path, sam_checkpoint_path):\n",
        "    self.pose_model = YOLO(pose_model_path)\n",
        "    sam = sam_model_registry[\"default\"](checkpoint=sam_checkpoint_path)\n",
        "    sam = sam.to(device='cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    self.mask_predictor = SamPredictor(sam)\n",
        "\n",
        "#defining function for color masks\n",
        "  @staticmethod\n",
        "  def apply_color_masks(image, masks):\n",
        "    for i, mask in enumerate(masks):\n",
        "      mask = cv2.cvtColor((mask[0].squeeze() * 255).astype('uint8'), cv2.COLOR_GRAY2BGR)\n",
        "      color_mask = np.zeros_like(image)\n",
        "      color_mask[:,:] = Marmo4b.COLORS[i]\n",
        "      color_mask[mask == 0] = 0\n",
        "\n",
        "      image = cv2.addWeighted(image, 0.7, color_mask, 0.3, 0)\n",
        "    return image\n",
        "\n",
        "#defining function for joint points\n",
        "  @staticmethod\n",
        "  def draw_joint(image, points):\n",
        "    for i in range(len(points)):\n",
        "      for x, y in points[i]:\n",
        "        image = cv2.circle(image, (int(x), int(y)), 2, Marmo4b.COLORS[i], -1)\n",
        "    return image\n",
        "\n",
        "#defining function to draw skeleton connections\n",
        "  @staticmethod\n",
        "  def draw_skeleton(image, points, masks):\n",
        "    overlay = image.copy()\n",
        "    for i in range(len(points)):\n",
        "      mask = masks[i]\n",
        "      point = points[i]\n",
        "      for start_i, end_i in Marmo4b.SKELETON:\n",
        "        if mask[start_i] and mask[end_i]:\n",
        "          # If confident, then draw\n",
        "          overlay = cv2.line(overlay, tuple(point[start_i].astype(int)), tuple(point[end_i].astype(int)), Marmo4b.COLORS[i], 2)\n",
        "\n",
        "    image = cv2.addWeighted(image, 0.3, overlay, 0.7, 0)\n",
        "\n",
        "    return image\n",
        "\n",
        "#defining funtion for drawing bounding boxes\n",
        "  @staticmethod\n",
        "  def is_within_box(bboxes, points):\n",
        "    mask = np.zeros((points.shape[0], points.shape[1]), dtype=bool)\n",
        "    for bbox in bboxes:\n",
        "      in_x = (points[:, :, 0] >= bbox[0]) & (points[:, :, 0] <= bbox[2])\n",
        "      in_y = (points[:, :, 1] >= bbox[1]) & (points[:, :, 1] <= bbox[3])\n",
        "\n",
        "      mask = mask | (in_x & in_y)\n",
        "    return mask\n",
        "\n",
        "#defining function for using both models together\n",
        "  def predict_image(self, image, mode=\"points\"):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      image (OpenCV image object)\n",
        "    \"\"\"\n",
        "    if image is None:\n",
        "      raise ValueError(\"Image not provided\")\n",
        "\n",
        "    MODES = [\"points\", \"box\"]\n",
        "    if not mode in MODES:\n",
        "      raise ValueError(f\"Invalid mode of operation: use {' '.join(MODES)}\")\n",
        "\n",
        "    pose_result = self.pose_model(image)[0]\n",
        "\n",
        "    joint_points = pose_result.keypoints.xy.cpu().numpy()\n",
        "    boxes = pose_result.boxes.xyxy.cpu().numpy()\n",
        "    point_confidences = pose_result.keypoints.conf.cpu().numpy()\n",
        "    # box_mask = is_within_box(boxes, joint_points)\n",
        "    confidence_mask = point_confidences > self.CONFIDENCE_THRESHOLD\n",
        "    zero_mask = ~np.all(joint_points[:, :, :] == np.array([0, 0]), axis=2)\n",
        "    point_mask = zero_mask & confidence_mask\n",
        "    input_points = np.where(np.expand_dims(point_mask, axis=2), joint_points, 0)\n",
        "\n",
        "    self.mask_predictor.set_image(image)\n",
        "\n",
        "    if mode == \"points\":\n",
        "      input_labels = point_mask.astype(int)\n",
        "      sam_masks = [self.mask_predictor.predict(point_coords=point, point_labels=label, multimask_output=False) for point, label in zip(input_points, input_labels)]\n",
        "\n",
        "    if mode == \"box\":\n",
        "      sam_masks = [self.mask_predictor.predict(box=box, multimask_output=False) for box in boxes]\n",
        "\n",
        "    image = self.apply_color_masks(image, sam_masks)\n",
        "    image = self.draw_joint(image, input_points)\n",
        "    image = self.draw_skeleton(image, joint_points, point_mask)\n",
        "\n",
        "    return image\n",
        "\n",
        "  def predict_images(self, image_paths, save_path, mode=\"points\"):\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "    for image_path in tqdm(image_paths):\n",
        "      image = cv2.imread(image_path)\n",
        "      result = self.predict_image(image, mode)\n",
        "      cv2.imwrite(save_path + image_path.split(\"/\")[-1], result)"
      ],
      "metadata": {
        "id": "vPDTXbBYoAT5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initializing the model\n",
        "model = Marmo4b(\"marmo-4b/marmo-pose-one-class.pt\", \"/content/sam_vit_h.pth\")"
      ],
      "metadata": {
        "id": "7d2vAnx11nqi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#single image demo\n",
        "image = cv2.imread(\"/content/test/marmoset-dlc-2021-05-07/labeled-data/reachingvideo1/img01310.png\")\n",
        "result = model.predict_image(image, mode=\"box\")\n",
        "cv2.imwrite(\"processed.png\", result)"
      ],
      "metadata": {
        "id": "HsqB-1zp1-Fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#setting image path for prediction\n",
        "image_paths = glob.glob(\"/content/test/marmoset-dlc-2021-05-07/labeled-data/reachingvideo1/*.png\")\n",
        "#batch prediction\n",
        "model.predict_images(image_paths, \"/content/predict/\", mode=\"box\")"
      ],
      "metadata": {
        "id": "Bd7woFIbaPSE",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#image to video conversion function\n",
        "def images_to_video(image_directory, video_name, fps=30, target_size=(500, 500)):\n",
        "\n",
        "  image_paths = glob.glob(image_directory + \"*.png\")\n",
        "  image_paths.sort()\n",
        "\n",
        "  fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "  video = cv2.VideoWriter(video_name, fourcc, fps, target_size)\n",
        "\n",
        "  for image_path in image_paths:\n",
        "    frame = cv2.imread(image_path)\n",
        "    resized_frame = cv2.resize(frame, target_size)\n",
        "    video.write(resized_frame)\n",
        "\n",
        "  video.release()"
      ],
      "metadata": {
        "id": "zUMJRtFGYxie"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert image to video\n",
        "images_to_video(\"/content/predict/\", \"myvideo.mp4\")"
      ],
      "metadata": {
        "id": "f3L9ilNimIQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part Three: Implementing Interface"
      ],
      "metadata": {
        "id": "C-AoDo2F5X6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#installing Gradio via pip\n",
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKJvLdXU5Zvr",
        "outputId": "2de3370f-0626-4158-eb69-772b18393bd7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (4.43.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi<0.113.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.112.4)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.4.0)\n",
            "Requirement already satisfied: gradio-client==1.3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.3.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.24.6)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.4)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.8.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.9)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.6.4)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.7)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.30.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (2024.6.1)\n",
            "Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (12.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.8)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi<0.113.0->gradio) (0.38.4)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.15.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.20.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.8.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loading prerequisites\n",
        "import gradio as gr\n",
        "from PIL import Image\n",
        "import os"
      ],
      "metadata": {
        "id": "UILGUUwv5cUK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining function for batch prediction (multiple images)\n",
        "def batch_predict(image_paths, mode, model):\n",
        "  temp_dir = \"temp_results/\"\n",
        "  os.makedirs(temp_dir, exist_ok=True)\n",
        "\n",
        "  model.predict_images(image_paths, temp_dir, mode)\n",
        "  result_paths = glob.glob(temp_dir + \"*\")\n",
        "\n",
        "  return [Image.open(result_path) for result_path in result_paths]\n",
        "\n",
        "#defining gradio's interface\n",
        "def launch_interface(model):\n",
        "  gr.Interface(\n",
        "      fn=lambda image_list, mode: batch_predict(image_list, mode, model),\n",
        "      inputs=[\n",
        "          gr.File(label=\"Upload images\", file_count=\"multiple\", type=\"filepath\"),\n",
        "          gr.Radio(choices=[\"points\", \"box\"], label=\"Segmentation mode\"),\n",
        "      ],\n",
        "      outputs=gr.Gallery(label=\"Processed Images\"),\n",
        "      title=\"Pose Estimation and Instance Segmentation\",\n",
        "      description=\"Upload images and select the mode (points or box) to predict poses and masks.\"\n",
        "  ).launch(debug=True)"
      ],
      "metadata": {
        "id": "XbNj4nPW5jlh"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#launch gradio webapp\n",
        "launch_interface(model)"
      ],
      "metadata": {
        "id": "JE589CJmDImD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 764
        },
        "outputId": "229b1836-eafb-4417-ffe8-af4fa89f2c44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://93e53a20a7a982a014.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://93e53a20a7a982a014.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 640x640 1 marmoset, 9.8ms\n",
            "Speed: 3.2ms preprocess, 9.8ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:02<00:00,  2.21s/it]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 640x640 1 marmoset, 8.4ms\n",
            "Speed: 2.7ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:02<00:00,  2.31s/it]\n"
          ]
        }
      ]
    }
  ]
}